# Attention is All You Need



## Links

- [arxiv: Attention is All You Need](https://arxiv.org/abs/1706.03762)
- [公众号文章: Attention Is All You Need (Transformer) 论文精读](https://mp.weixin.qq.com/s/sU2uK2kQVJxzXeVNTmvfYg)

## 核心内容

- 注意力机制(Attention Mechanism)
    - **自注意力机制**(Self-Attention Mechanism)
- **“注意力机制”**（Attention Mechanism）可以取代传统的基于递归神经网络（RNN）或卷积神经网络（CNN）的结构，在自然语言处理和序列任务中表现出更高的效率和效果。具体来说, Transformer架构完全依赖注意力机制进行建模，而不需要RNN等传统结构，显著提高了并行化能力，提升了训练速度。
- 其实，“注意力”这个名字取得非常不易于理解。这个机制应该叫做“全局信息查询”。做一次“注意力”计算，其实就跟去数据库了做了一次查询一样。
- **Transformer架构**
- 位置编码(Positional Encoding)
- 提升训练并行度



![Refer to caption](https://wwfyde.oss-cn-hangzhou.aliyuncs.com/images/202409132238646.png)